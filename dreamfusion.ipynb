{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d62ea7-7c94-408a-a387-457809db3e20",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4a69475-2b70-4f36-88ea-5c056a2504e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer, logging\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler, DDIMScheduler, StableDiffusionPipeline\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "\n",
    "from torch.cuda.amp import custom_bwd, custom_fwd\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from os.path import isfile\n",
    "from pathlib import Path\n",
    "\n",
    "# suppress partial model loading warning\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e429502-9b24-48a3-8770-fccf33bdf58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d098512-6856-4508-8d26-2d9137226a1c",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aad7055-5adf-486c-bbe8-94a29af0f8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda', index = 1)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "046c3025-2e52-41c4-bf68-af7333cbb7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp16 = False\n",
    "vram_o = True\n",
    "hf_key = 'stabilityai/stable-diffusion-2-1-base'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cf62b2-d089-4a44-bd6f-00e3f4fe9f78",
   "metadata": {},
   "source": [
    "# Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49b06c7f-f616-4e55-b566-3c1dbf15e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perpendicular_component(x, y):\n",
    "    assert x.shape == y.shape\n",
    "    return x - ((torch.mul(x, y).sum())/max(torch.norm(y)**2, 1e-6)) * y\n",
    "\n",
    "\n",
    "def batch_get_perpendicular_component(x, y):\n",
    "    assert x.shape == y.shape\n",
    "    result = []\n",
    "    for i in range(x.shape[0]):\n",
    "        result.append(get_perpendicular_component(x[i], y[i]))\n",
    "    return torch.stack(result)\n",
    "\n",
    "def weighted_perpendicular_aggregator(delta_noise_preds, weights, batch_size):\n",
    "    \"\"\" \n",
    "    Notes: \n",
    "     - weights: an array with the weights for combining the noise predictions\n",
    "     - delta_noise_preds: [B x K, 4, 64, 64], K = max_prompts_per_dir\n",
    "    \"\"\"\n",
    "    delta_noise_preds = delta_noise_preds.split(batch_size, dim=0) # K x [B, 4, 64, 64]\n",
    "    weights = weights.split(batch_size, dim=0) # K x [B]\n",
    "\n",
    "    assert torch.all(weights[0] == 1.0)\n",
    "\n",
    "    main_positive = delta_noise_preds[0] # [B, 4, 64, 64]\n",
    "\n",
    "    accumulated_output = torch.zeros_like(main_positive)\n",
    "    for i, complementary_noise_pred in enumerate(delta_noise_preds[1:], start=1):\n",
    "        idx_non_zero = torch.abs(weights[i]) > 1e-4\n",
    "        if sum(idx_non_zero) == 0:\n",
    "            continue\n",
    "        accumulated_output[idx_non_zero] += weights[i][idx_non_zero].reshape(-1, 1, 1, 1) * batch_get_perpendicular_component(complementary_noise_pred[idx_non_zero], main_positive[idx_non_zero])\n",
    "    \n",
    "    assert accumulated_output.shape == main_positive.shape, f\"{accumulated_output.shape = }, {main_positive.shape = }\"\n",
    "\n",
    "    return accumulated_output + main_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e144693b-7898-45bf-9eff-424e8e30b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"key\n",
    "stabilityai/stable-diffusion-2-1-base\n",
    "stabilityai/stable-diffusion-2-base\n",
    "runwayml/stable-diffusion-v1-5\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class StableDiffusion(nn.Module):\n",
    "    def __init__(self, device, fp16, vram_o, hf_key='stabilityai/stable-diffusion-2-1-base', t_range=[0.02, 0.98]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        print(f'[INFO] loading stable diffusion...')\n",
    "\n",
    "        self.precision_t = torch.float16 if fp16 else torch.float32\n",
    "\n",
    "        # Create model\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(hf_key, torch_dtype=self.precision_t, force_download=True)\n",
    "\n",
    "        if vram_o:\n",
    "            pipe.enable_sequential_cpu_offload()\n",
    "            pipe.enable_vae_slicing()\n",
    "            pipe.unet.to(memory_format=torch.channels_last)\n",
    "            pipe.enable_attention_slicing(1)\n",
    "        else:\n",
    "            pipe.to(device)\n",
    "\n",
    "        self.vae = pipe.vae\n",
    "        self.tokenizer = pipe.tokenizer\n",
    "        self.text_encoder = pipe.text_encoder\n",
    "        self.unet = pipe.unet\n",
    "\n",
    "        self.scheduler = DDIMScheduler.from_pretrained(hf_key, subfolder=\"scheduler\", torch_dtype=self.precision_t, force_download=True)\n",
    "\n",
    "        del pipe\n",
    "\n",
    "        self.num_train_timesteps = self.scheduler.config.num_train_timesteps\n",
    "        self.min_step = int(self.num_train_timesteps * t_range[0])\n",
    "        self.max_step = int(self.num_train_timesteps * t_range[1])\n",
    "        self.alphas = self.scheduler.alphas_cumprod.to(self.device) # for convenience\n",
    "\n",
    "        print(f'[INFO] loaded stable diffusion!')\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_text_embeds(self, prompt: str):\n",
    "\n",
    "        inputs = self.tokenizer(prompt, padding='max_length', max_length=self.tokenizer.model_max_length, return_tensors='pt')\n",
    "        embeddings = self.text_encoder(inputs.input_ids.to(self.device))[0]\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "    def train_step(self, text_embeddings, pred_rgb, guidance_scale=100, as_latent=False, grad_scale=1, save_guidance_path:Path=None):\n",
    "\n",
    "        if as_latent:\n",
    "            latents = F.interpolate(pred_rgb, (64, 64), mode='bilinear', align_corners=False) * 2 - 1\n",
    "        else:\n",
    "            # interp to 512x512 to be fed into vae.\n",
    "            pred_rgb_512 = F.interpolate(pred_rgb, (512, 512), mode='bilinear', align_corners=False)\n",
    "            # encode image into latents with vae, requires grad!\n",
    "            latents = self.encode_imgs(pred_rgb_512)\n",
    "\n",
    "        # timestep ~ U(0.02, 0.98) to avoid very high/low noise level\n",
    "        t = torch.randint(self.min_step, self.max_step + 1, (latents.shape[0],), dtype=torch.long, device=self.device)\n",
    "\n",
    "        # predict the noise residual with unet, NO grad!\n",
    "        with torch.no_grad():\n",
    "            # add noise\n",
    "            noise = torch.randn_like(latents)\n",
    "            latents_noisy = self.scheduler.add_noise(latents, noise, t)\n",
    "            # pred noise\n",
    "            latent_model_input = torch.cat([latents_noisy] * 2)\n",
    "            tt = torch.cat([t] * 2)\n",
    "            noise_pred = self.unet(latent_model_input, tt, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "            # perform guidance (high scale from paper!)\n",
    "            noise_pred_uncond, noise_pred_pos = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_pos - noise_pred_uncond)\n",
    "\n",
    "        # w(t), sigma_t^2\n",
    "        w = (1 - self.alphas[t])\n",
    "        grad = grad_scale * w[:, None, None, None] * (noise_pred - noise)\n",
    "        grad = torch.nan_to_num(grad)\n",
    "\n",
    "        if save_guidance_path:\n",
    "            with torch.no_grad():\n",
    "                if as_latent:\n",
    "                    pred_rgb_512 = self.decode_latents(latents)\n",
    "\n",
    "                # visualize predicted denoised image\n",
    "                # The following block of code is equivalent to `predict_start_from_noise`...\n",
    "                # see zero123_utils.py's version for a simpler implementation.\n",
    "                alphas = self.scheduler.alphas.to(latents)\n",
    "                total_timesteps = self.max_step - self.min_step + 1\n",
    "                index = total_timesteps - t.to(latents.device) - 1 \n",
    "                b = len(noise_pred)\n",
    "                a_t = alphas[index].reshape(b,1,1,1).to(self.device)\n",
    "                sqrt_one_minus_alphas = torch.sqrt(1 - alphas)\n",
    "                sqrt_one_minus_at = sqrt_one_minus_alphas[index].reshape((b,1,1,1)).to(self.device)                \n",
    "                pred_x0 = (latents_noisy - sqrt_one_minus_at * noise_pred) / a_t.sqrt() # current prediction for x_0\n",
    "                result_hopefully_less_noisy_image = self.decode_latents(pred_x0.to(latents.type(self.precision_t)))\n",
    "\n",
    "                # visualize noisier image\n",
    "                result_noisier_image = self.decode_latents(latents_noisy.to(pred_x0).type(self.precision_t))\n",
    "\n",
    "                # TODO: also denoise all-the-way\n",
    "\n",
    "                # all 3 input images are [1, 3, H, W], e.g. [1, 3, 512, 512]\n",
    "                viz_images = torch.cat([pred_rgb_512, result_noisier_image, result_hopefully_less_noisy_image],dim=0)\n",
    "                save_image(viz_images, save_guidance_path)\n",
    "\n",
    "        targets = (latents - grad).detach()\n",
    "        loss = 0.5 * F.mse_loss(latents.float(), targets, reduction='sum') / latents.shape[0]\n",
    "\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def train_step_perpneg(self, text_embeddings, weights, pred_rgb, guidance_scale=100, as_latent=False, grad_scale=1, save_guidance_path:Path=None):\n",
    "\n",
    "        B = pred_rgb.shape[0]\n",
    "        K = (text_embeddings.shape[0] // B) - 1 # maximum number of prompts       \n",
    "\n",
    "        if as_latent:\n",
    "            latents = F.interpolate(pred_rgb, (64, 64), mode='bilinear', align_corners=False) * 2 - 1\n",
    "        else:\n",
    "            # interp to 512x512 to be fed into vae.\n",
    "            pred_rgb_512 = F.interpolate(pred_rgb, (512, 512), mode='bilinear', align_corners=False)\n",
    "            # encode image into latents with vae, requires grad!\n",
    "            latents = self.encode_imgs(pred_rgb_512)\n",
    "\n",
    "        # timestep ~ U(0.02, 0.98) to avoid very high/low noise level\n",
    "        t = torch.randint(self.min_step, self.max_step + 1, (latents.shape[0],), dtype=torch.long, device=self.device)\n",
    "\n",
    "        # predict the noise residual with unet, NO grad!\n",
    "        with torch.no_grad():\n",
    "            # add noise\n",
    "            noise = torch.randn_like(latents)\n",
    "            latents_noisy = self.scheduler.add_noise(latents, noise, t)\n",
    "            # pred noise\n",
    "            latent_model_input = torch.cat([latents_noisy] * (1 + K))\n",
    "            tt = torch.cat([t] * (1 + K))\n",
    "            unet_output = self.unet(latent_model_input, tt, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "            # perform guidance (high scale from paper!)\n",
    "            noise_pred_uncond, noise_pred_text = unet_output[:B], unet_output[B:]\n",
    "            delta_noise_preds = noise_pred_text - noise_pred_uncond.repeat(K, 1, 1, 1)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * weighted_perpendicular_aggregator(delta_noise_preds, weights, B)            \n",
    "\n",
    "        # w(t), sigma_t^2\n",
    "        w = (1 - self.alphas[t])\n",
    "        grad = grad_scale * w[:, None, None, None] * (noise_pred - noise)\n",
    "        grad = torch.nan_to_num(grad)\n",
    "\n",
    "        if save_guidance_path:\n",
    "            with torch.no_grad():\n",
    "                if as_latent:\n",
    "                    pred_rgb_512 = self.decode_latents(latents)\n",
    "\n",
    "                # visualize predicted denoised image\n",
    "                # The following block of code is equivalent to `predict_start_from_noise`...\n",
    "                # see zero123_utils.py's version for a simpler implementation.\n",
    "                alphas = self.scheduler.alphas.to(latents)\n",
    "                total_timesteps = self.max_step - self.min_step + 1\n",
    "                index = total_timesteps - t.to(latents.device) - 1 \n",
    "                b = len(noise_pred)\n",
    "                a_t = alphas[index].reshape(b,1,1,1).to(self.device)\n",
    "                sqrt_one_minus_alphas = torch.sqrt(1 - alphas)\n",
    "                sqrt_one_minus_at = sqrt_one_minus_alphas[index].reshape((b,1,1,1)).to(self.device)                \n",
    "                pred_x0 = (latents_noisy - sqrt_one_minus_at * noise_pred) / a_t.sqrt() # current prediction for x_0\n",
    "                result_hopefully_less_noisy_image = self.decode_latents(pred_x0.to(latents.type(self.precision_t)))\n",
    "\n",
    "                # visualize noisier image\n",
    "                result_noisier_image = self.decode_latents(latents_noisy.to(pred_x0).type(self.precision_t))\n",
    "\n",
    "\n",
    "\n",
    "                # all 3 input images are [1, 3, H, W], e.g. [1, 3, 512, 512]\n",
    "                viz_images = torch.cat([pred_rgb_512, result_noisier_image, result_hopefully_less_noisy_image],dim=0)\n",
    "                save_image(viz_images, save_guidance_path)\n",
    "\n",
    "        targets = (latents - grad).detach()\n",
    "        loss = 0.5 * F.mse_loss(latents.float(), targets, reduction='sum') / latents.shape[0]\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def produce_latents(self, text_embeddings, height=512, width=512, num_inference_steps=50, guidance_scale=7.5, latents=None):\n",
    "\n",
    "        if latents is None:\n",
    "            latents = torch.randn((text_embeddings.shape[0] // 2, self.unet.config.in_channels, height // 8, width // 8), device=self.device)\n",
    "\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        for i, t in enumerate(self.scheduler.timesteps):\n",
    "            # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "            # predict the noise residual\n",
    "            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings)['sample']\n",
    "\n",
    "            # perform guidance\n",
    "            noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "\n",
    "            # compute the previous noisy sample x_t -> x_t-1\n",
    "            latents = self.scheduler.step(noise_pred, t, latents)['prev_sample']\n",
    "\n",
    "        return latents\n",
    "\n",
    "    def decode_latents(self, latents):\n",
    "\n",
    "        latents = 1 / self.vae.config.scaling_factor * latents\n",
    "\n",
    "        imgs = self.vae.decode(latents).sample\n",
    "        imgs = (imgs / 2 + 0.5).clamp(0, 1)\n",
    "\n",
    "        return imgs\n",
    "\n",
    "    def encode_imgs(self, imgs):\n",
    "        # imgs: [B, 3, H, W]\n",
    "\n",
    "        imgs = 2 * imgs - 1\n",
    "\n",
    "        posterior = self.vae.encode(imgs).latent_dist\n",
    "        latents = posterior.sample() * self.vae.config.scaling_factor\n",
    "\n",
    "        return latents\n",
    "\n",
    "    def prompt_to_img(self, prompts, negative_prompts='', height=512, width=512, num_inference_steps=50, guidance_scale=7.5, latents=None):\n",
    "\n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts]\n",
    "\n",
    "        if isinstance(negative_prompts, str):\n",
    "            negative_prompts = [negative_prompts]\n",
    "\n",
    "        # Prompts -> text embeds\n",
    "        pos_embeds = self.get_text_embeds(prompts) # [1, 77, 768]\n",
    "        neg_embeds = self.get_text_embeds(negative_prompts)\n",
    "        text_embeds = torch.cat([neg_embeds, pos_embeds], dim=0) # [2, 77, 768]\n",
    "\n",
    "        # Text embeds -> img latents\n",
    "        latents = self.produce_latents(text_embeds, height=height, width=width, latents=latents, num_inference_steps=num_inference_steps, guidance_scale=guidance_scale) # [1, 4, 64, 64]\n",
    "\n",
    "        # Img latents -> imgs\n",
    "        imgs = self.decode_latents(latents) # [1, 3, 512, 512]\n",
    "\n",
    "        # Img to Numpy\n",
    "        imgs = imgs.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "        imgs = (imgs * 255).round().astype('uint8')\n",
    "\n",
    "        return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2922a322-f79d-4049-8ac9-6a7a3c63a8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading stable diffusion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/home/khoidh/git/simple-dream-fusion/.env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b93236ac0b4116b071aa9cec36805c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/543 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b45a388eed4cdf94a6110d6253b35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a7e6086fc84869a389c63eb2d602e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c9f6f4fc6f49f0b8298f8750dd6f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler/scheduler_config.json:   0%|          | 0.00/346 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loaded stable diffusion!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/home/khoidh/git/simple-dream-fusion/.env/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m W \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[1;32m      7\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m----> 9\u001b[0m imgs \u001b[38;5;241m=\u001b[39m \u001b[43mguidance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_to_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(imgs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[0;32mIn[6], line 247\u001b[0m, in \u001b[0;36mStableDiffusion.prompt_to_img\u001b[0;34m(self, prompts, negative_prompts, height, width, num_inference_steps, guidance_scale, latents)\u001b[0m\n\u001b[1;32m    244\u001b[0m latents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproduce_latents(text_embeds, height\u001b[38;5;241m=\u001b[39mheight, width\u001b[38;5;241m=\u001b[39mwidth, latents\u001b[38;5;241m=\u001b[39mlatents, num_inference_steps\u001b[38;5;241m=\u001b[39mnum_inference_steps, guidance_scale\u001b[38;5;241m=\u001b[39mguidance_scale) \u001b[38;5;66;03m# [1, 4, 64, 64]\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# Img latents -> imgs\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m imgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_latents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [1, 3, 512, 512]\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Img to Numpy\u001b[39;00m\n\u001b[1;32m    250\u001b[0m imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[6], line 215\u001b[0m, in \u001b[0;36mStableDiffusion.decode_latents\u001b[0;34m(self, latents)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_latents\u001b[39m(\u001b[38;5;28mself\u001b[39m, latents):\n\u001b[1;32m    213\u001b[0m     latents \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mscaling_factor \u001b[38;5;241m*\u001b[39m latents\n\u001b[0;32m--> 215\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m    216\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m (imgs \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m imgs\n",
      "File \u001b[0;32m~/git/simple-dream-fusion/.env/lib/python3.10/site-packages/diffusers/utils/accelerate_utils.py:46\u001b[0m, in \u001b[0;36mapply_forward_hook.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hf_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpre_forward(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/simple-dream-fusion/.env/lib/python3.10/site-packages/diffusers/models/autoencoders/autoencoder_kl.py:304\u001b[0m, in \u001b[0;36mAutoencoderKL.decode\u001b[0;34m(self, z, return_dict, generator)\u001b[0m\n\u001b[1;32m    302\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(decoded_slices)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (decoded,)\n",
      "File \u001b[0;32m~/git/simple-dream-fusion/.env/lib/python3.10/site-packages/diffusers/models/autoencoders/autoencoder_kl.py:275\u001b[0m, in \u001b[0;36mAutoencoderKL._decode\u001b[0;34m(self, z, return_dict)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtiled_decode(z, return_dict\u001b[38;5;241m=\u001b[39mreturn_dict)\n\u001b[1;32m    274\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_quant_conv(z)\n\u001b[0;32m--> 275\u001b[0m dec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (dec,)\n",
      "File \u001b[0;32m~/git/simple-dream-fusion/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/simple-dream-fusion/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/git/simple-dream-fusion/.env/lib/python3.10/site-packages/diffusers/models/autoencoders/vae.py:338\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, sample, latent_embeds)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;66;03m# up\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m up_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_blocks:\n\u001b[0;32m--> 338\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[43mup_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# post-process\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m latent_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/git/simple-dream-fusion/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/simple-dream-fusion/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/git/simple-dream-fusion/.env/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_blocks.py:2737\u001b[0m, in \u001b[0;36mUpDecoderBlock2D.forward\u001b[0;34m(self, hidden_states, temb)\u001b[0m\n\u001b[1;32m   2735\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mFloatTensor, temb: Optional[torch\u001b[38;5;241m.\u001b[39mFloatTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[1;32m   2736\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m resnet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnets:\n\u001b[0;32m-> 2737\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2739\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2740\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m upsampler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsamplers:\n",
      "File \u001b[0;32m~/git/simple-dream-fusion/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/simple-dream-fusion/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/git/simple-dream-fusion/.env/lib/python3.10/site-packages/diffusers/models/resnet.py:371\u001b[0m, in \u001b[0;36mResnetBlock2D.forward\u001b[0;34m(self, input_tensor, temb, *args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnonlinearity(hidden_states)\n\u001b[1;32m    370\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m--> 371\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_shortcut \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    374\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_shortcut(input_tensor)\n",
      "File \u001b[0;32m~/git/simple-dream-fusion/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/simple-dream-fusion/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/git/simple-dream-fusion/.env/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/git/simple-dream-fusion/.env/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/simple-dream-fusion/.env/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "guidance = StableDiffusion(device=device, fp16=fp16, vram_o=vram_o, hf_key=hf_key)\n",
    "\n",
    "prompt = \"my girl\"\n",
    "negative = ''\n",
    "H = 512\n",
    "W = 512\n",
    "steps = 50\n",
    "\n",
    "imgs = guidance.prompt_to_img(prompt, negative, H, W, steps)\n",
    "plt.imshow(imgs[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d0e57b-8a4c-4340-a525-3645c7e8cd4f",
   "metadata": {},
   "source": [
    "# Nerf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b8cf53-7f55-4938-8d93-8a15c6c3f977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbfab8d3-01af-4a78-be0d-61ea9d06055b",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6731cd7-dc84-47cf-9481-14663d6407be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
